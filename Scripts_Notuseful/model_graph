digraph {
	graph [size="62.699999999999996,62.699999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139998120774368 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	139998005364960 [label=AddmmBackward0]
	139998005364624 -> 139998005364960
	139998005925168 [label="fc.bias
 (10)" fillcolor=lightblue]
	139998005925168 -> 139998005364624
	139998005364624 [label=AccumulateGrad]
	139998005365392 -> 139998005364960
	139998005365392 [label=ViewBackward0]
	139998005365200 -> 139998005365392
	139998005365200 [label=AvgPool2DBackward0]
	139998005364912 -> 139998005365200
	139998005364912 [label=ReluBackward0]
	139998005364816 -> 139998005364912
	139998005364816 [label=AddBackward0]
	139998005364720 -> 139998005364816
	139998005364720 [label=NativeBatchNormBackward0]
	139998005364480 -> 139998005364720
	139998005364480 [label=ConvolutionBackward0]
	139998005364288 -> 139998005364480
	139998005364288 [label=ReluBackward0]
	139998005364144 -> 139998005364288
	139998005364144 [label=NativeBatchNormBackward0]
	139998005364048 -> 139998005364144
	139998005364048 [label=ConvolutionBackward0]
	139998005364768 -> 139998005364048
	139998005364768 [label=ReluBackward0]
	139998005363856 -> 139998005364768
	139998005363856 [label=AddBackward0]
	139998005363760 -> 139998005363856
	139998005363760 [label=NativeBatchNormBackward0]
	139998005363136 -> 139998005363760
	139998005363136 [label=ConvolutionBackward0]
	139998005363280 -> 139998005363136
	139998005363280 [label=ReluBackward0]
	139998005363184 -> 139998005363280
	139998005363184 [label=NativeBatchNormBackward0]
	139998006467296 -> 139998005363184
	139998006467296 [label=ConvolutionBackward0]
	139998005363808 -> 139998006467296
	139998005363808 [label=ReluBackward0]
	139998105158176 -> 139998005363808
	139998105158176 [label=AddBackward0]
	139998105010032 -> 139998105158176
	139998105010032 [label=NativeBatchNormBackward0]
	139998104765392 -> 139998105010032
	139998104765392 [label=ConvolutionBackward0]
	139998005657408 -> 139998104765392
	139998005657408 [label=ReluBackward0]
	139998005657120 -> 139998005657408
	139998005657120 [label=NativeBatchNormBackward0]
	139998005656928 -> 139998005657120
	139998005656928 [label=ConvolutionBackward0]
	139998005656544 -> 139998005656928
	139998005656544 [label=ReluBackward0]
	139998005656448 -> 139998005656544
	139998005656448 [label=AddBackward0]
	139998005656352 -> 139998005656448
	139998005656352 [label=NativeBatchNormBackward0]
	139998005655968 -> 139998005656352
	139998005655968 [label=ConvolutionBackward0]
	139998005655584 -> 139998005655968
	139998005655584 [label=ReluBackward0]
	139998005655488 -> 139998005655584
	139998005655488 [label=NativeBatchNormBackward0]
	139998005655392 -> 139998005655488
	139998005655392 [label=ConvolutionBackward0]
	139998005656400 -> 139998005655392
	139998005656400 [label=ReluBackward0]
	139998005654912 -> 139998005656400
	139998005654912 [label=AddBackward0]
	139998005654720 -> 139998005654912
	139998005654720 [label=NativeBatchNormBackward0]
	139998005654480 -> 139998005654720
	139998005654480 [label=ConvolutionBackward0]
	139998005654048 -> 139998005654480
	139998005654048 [label=ReluBackward0]
	139998005653952 -> 139998005654048
	139998005653952 [label=NativeBatchNormBackward0]
	139998005653760 -> 139998005653952
	139998005653760 [label=ConvolutionBackward0]
	139998005654624 -> 139998005653760
	139998005654624 [label=ReluBackward0]
	139998005653376 -> 139998005654624
	139998005653376 [label=AddBackward0]
	139998005653184 -> 139998005653376
	139998005653184 [label=NativeBatchNormBackward0]
	139998005652704 -> 139998005653184
	139998005652704 [label=ConvolutionBackward0]
	139998005652560 -> 139998005652704
	139998005652560 [label=ReluBackward0]
	139998005652416 -> 139998005652560
	139998005652416 [label=NativeBatchNormBackward0]
	139998005652224 -> 139998005652416
	139998005652224 [label=ConvolutionBackward0]
	139998005651840 -> 139998005652224
	139998005651840 [label=ReluBackward0]
	139998005651600 -> 139998005651840
	139998005651600 [label=AddBackward0]
	139998005651360 -> 139998005651600
	139998005651360 [label=NativeBatchNormBackward0]
	139998005651264 -> 139998005651360
	139998005651264 [label=ConvolutionBackward0]
	139998005650880 -> 139998005651264
	139998005650880 [label=ReluBackward0]
	139998005650640 -> 139998005650880
	139998005650640 [label=NativeBatchNormBackward0]
	139998005650400 -> 139998005650640
	139998005650400 [label=ConvolutionBackward0]
	139998005651552 -> 139998005650400
	139998005651552 [label=ReluBackward0]
	139998005649824 -> 139998005651552
	139998005649824 [label=AddBackward0]
	139998005649776 -> 139998005649824
	139998005649776 [label=NativeBatchNormBackward0]
	139998005649632 -> 139998005649776
	139998005649632 [label=ConvolutionBackward0]
	139998005649344 -> 139998005649632
	139998005649344 [label=ReluBackward0]
	139998005648864 -> 139998005649344
	139998005648864 [label=NativeBatchNormBackward0]
	139998005648816 -> 139998005648864
	139998005648816 [label=ConvolutionBackward0]
	139998005649920 -> 139998005648816
	139998005649920 [label=ReluBackward0]
	139998005648288 -> 139998005649920
	139998005648288 [label=AddBackward0]
	139998005648096 -> 139998005648288
	139998005648096 [label=NativeBatchNormBackward0]
	139998005648000 -> 139998005648096
	139998005648000 [label=ConvolutionBackward0]
	139998005647712 -> 139998005648000
	139998005647712 [label=ReluBackward0]
	139998005647328 -> 139998005647712
	139998005647328 [label=NativeBatchNormBackward0]
	139998005647136 -> 139998005647328
	139998005647136 [label=ConvolutionBackward0]
	139998005648384 -> 139998005647136
	139998005648384 [label=ReluBackward0]
	139998005646800 -> 139998005648384
	139998005646800 [label=NativeBatchNormBackward0]
	139998005646560 -> 139998005646800
	139998005646560 [label=ConvolutionBackward0]
	139998005646176 -> 139998005646560
	139998005731840 [label="conv1.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
	139998005731840 -> 139998005646176
	139998005646176 [label=AccumulateGrad]
	139998005646752 -> 139998005646800
	139998005731760 [label="bn1.weight
 (16)" fillcolor=lightblue]
	139998005731760 -> 139998005646752
	139998005646752 [label=AccumulateGrad]
	139998005646944 -> 139998005646800
	139998005731920 [label="bn1.bias
 (16)" fillcolor=lightblue]
	139998005731920 -> 139998005646944
	139998005646944 [label=AccumulateGrad]
	139998005646896 -> 139998005647136
	139998005732000 [label="layer1.0.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139998005732000 -> 139998005646896
	139998005646896 [label=AccumulateGrad]
	139998005647424 -> 139998005647328
	139998005732480 [label="layer1.0.bn1.weight
 (16)" fillcolor=lightblue]
	139998005732480 -> 139998005647424
	139998005647424 [label=AccumulateGrad]
	139998005647520 -> 139998005647328
	139998005732560 [label="layer1.0.bn1.bias
 (16)" fillcolor=lightblue]
	139998005732560 -> 139998005647520
	139998005647520 [label=AccumulateGrad]
	139998005647760 -> 139998005648000
	139998005733040 [label="layer1.0.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139998005733040 -> 139998005647760
	139998005647760 [label=AccumulateGrad]
	139998005647904 -> 139998005648096
	139998005733120 [label="layer1.0.bn2.weight
 (16)" fillcolor=lightblue]
	139998005733120 -> 139998005647904
	139998005647904 [label=AccumulateGrad]
	139998005648192 -> 139998005648096
	139998005733200 [label="layer1.0.bn2.bias
 (16)" fillcolor=lightblue]
	139998005733200 -> 139998005648192
	139998005648192 [label=AccumulateGrad]
	139998005648384 -> 139998005648288
	139998005648480 -> 139998005648816
	139998005733680 [label="layer1.1.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139998005733680 -> 139998005648480
	139998005648480 [label=AccumulateGrad]
	139998005648960 -> 139998005648864
	139998005733760 [label="layer1.1.bn1.weight
 (16)" fillcolor=lightblue]
	139998005733760 -> 139998005648960
	139998005648960 [label=AccumulateGrad]
	139998005649056 -> 139998005648864
	139998005733840 [label="layer1.1.bn1.bias
 (16)" fillcolor=lightblue]
	139998005733840 -> 139998005649056
	139998005649056 [label=AccumulateGrad]
	139998005649248 -> 139998005649632
	139998005734320 [label="layer1.1.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139998005734320 -> 139998005649248
	139998005649248 [label=AccumulateGrad]
	139998005649680 -> 139998005649776
	139998005734400 [label="layer1.1.bn2.weight
 (16)" fillcolor=lightblue]
	139998005734400 -> 139998005649680
	139998005649680 [label=AccumulateGrad]
	139998005649728 -> 139998005649776
	139998005734480 [label="layer1.1.bn2.bias
 (16)" fillcolor=lightblue]
	139998005734480 -> 139998005649728
	139998005649728 [label=AccumulateGrad]
	139998005649920 -> 139998005649824
	139998005650016 -> 139998005650400
	139998005734960 [label="layer1.2.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139998005734960 -> 139998005650016
	139998005650016 [label=AccumulateGrad]
	139998005650592 -> 139998005650640
	139998005735040 [label="layer1.2.bn1.weight
 (16)" fillcolor=lightblue]
	139998005735040 -> 139998005650592
	139998005650592 [label=AccumulateGrad]
	139998005650736 -> 139998005650640
	139998005735120 [label="layer1.2.bn1.bias
 (16)" fillcolor=lightblue]
	139998005735120 -> 139998005650736
	139998005650736 [label=AccumulateGrad]
	139998005650784 -> 139998005651264
	139998005735600 [label="layer1.2.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	139998005735600 -> 139998005650784
	139998005650784 [label=AccumulateGrad]
	139998005651168 -> 139998005651360
	139998005735680 [label="layer1.2.bn2.weight
 (16)" fillcolor=lightblue]
	139998005735680 -> 139998005651168
	139998005651168 [label=AccumulateGrad]
	139998005651456 -> 139998005651360
	139998005735760 [label="layer1.2.bn2.bias
 (16)" fillcolor=lightblue]
	139998005735760 -> 139998005651456
	139998005651456 [label=AccumulateGrad]
	139998005651552 -> 139998005651600
	139998005651744 -> 139998005652224
	139998005736880 [label="layer2.0.conv1.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	139998005736880 -> 139998005651744
	139998005651744 [label=AccumulateGrad]
	139998005652128 -> 139998005652416
	139998005736960 [label="layer2.0.bn1.weight
 (32)" fillcolor=lightblue]
	139998005736960 -> 139998005652128
	139998005652128 [label=AccumulateGrad]
	139998005652512 -> 139998005652416
	139998005737040 [label="layer2.0.bn1.bias
 (32)" fillcolor=lightblue]
	139998005737040 -> 139998005652512
	139998005652512 [label=AccumulateGrad]
	139998005652608 -> 139998005652704
	139998005737520 [label="layer2.0.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	139998005737520 -> 139998005652608
	139998005652608 [label=AccumulateGrad]
	139998005652992 -> 139998005653184
	139998005737600 [label="layer2.0.bn2.weight
 (32)" fillcolor=lightblue]
	139998005737600 -> 139998005652992
	139998005652992 [label=AccumulateGrad]
	139998005652896 -> 139998005653184
	139998005737680 [label="layer2.0.bn2.bias
 (32)" fillcolor=lightblue]
	139998005737680 -> 139998005652896
	139998005652896 [label=AccumulateGrad]
	139998005653088 -> 139998005653376
	139998005653088 [label=NativeBatchNormBackward0]
	139998005652032 -> 139998005653088
	139998005652032 [label=ConvolutionBackward0]
	139998005651840 -> 139998005652032
	139998005651696 -> 139998005652032
	139998005736240 [label="layer2.0.downsample.0.weight
 (32, 16, 1, 1)" fillcolor=lightblue]
	139998005736240 -> 139998005651696
	139998005651696 [label=AccumulateGrad]
	139998005652656 -> 139998005653088
	139998005736320 [label="layer2.0.downsample.1.weight
 (32)" fillcolor=lightblue]
	139998005736320 -> 139998005652656
	139998005652656 [label=AccumulateGrad]
	139998005652800 -> 139998005653088
	139998005736400 [label="layer2.0.downsample.1.bias
 (32)" fillcolor=lightblue]
	139998005736400 -> 139998005652800
	139998005652800 [label=AccumulateGrad]
	139998005653472 -> 139998005653760
	139998005738160 [label="layer2.1.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	139998005738160 -> 139998005653472
	139998005653472 [label=AccumulateGrad]
	139998005653664 -> 139998005653952
	139998005738240 [label="layer2.1.bn1.weight
 (32)" fillcolor=lightblue]
	139998005738240 -> 139998005653664
	139998005653664 [label=AccumulateGrad]
	139998005654144 -> 139998005653952
	139998005738320 [label="layer2.1.bn1.bias
 (32)" fillcolor=lightblue]
	139998005738320 -> 139998005654144
	139998005654144 [label=AccumulateGrad]
	139998005654336 -> 139998005654480
	139998005738800 [label="layer2.1.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	139998005738800 -> 139998005654336
	139998005654336 [label=AccumulateGrad]
	139998005654528 -> 139998005654720
	139998005738880 [label="layer2.1.bn2.weight
 (32)" fillcolor=lightblue]
	139998005738880 -> 139998005654528
	139998005654528 [label=AccumulateGrad]
	139998005654576 -> 139998005654720
	139998005738960 [label="layer2.1.bn2.bias
 (32)" fillcolor=lightblue]
	139998005738960 -> 139998005654576
	139998005654576 [label=AccumulateGrad]
	139998005654624 -> 139998005654912
	139998005655104 -> 139998005655392
	139998005739360 [label="layer2.2.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	139998005739360 -> 139998005655104
	139998005655104 [label=AccumulateGrad]
	139998005655440 -> 139998005655488
	139998005739440 [label="layer2.2.bn1.weight
 (32)" fillcolor=lightblue]
	139998005739440 -> 139998005655440
	139998005655440 [label=AccumulateGrad]
	139998005655680 -> 139998005655488
	139998005919808 [label="layer2.2.bn1.bias
 (32)" fillcolor=lightblue]
	139998005919808 -> 139998005655680
	139998005655680 [label=AccumulateGrad]
	139998005655872 -> 139998005655968
	139998005920288 [label="layer2.2.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	139998005920288 -> 139998005655872
	139998005655872 [label=AccumulateGrad]
	139998005656256 -> 139998005656352
	139998005920368 [label="layer2.2.bn2.weight
 (32)" fillcolor=lightblue]
	139998005920368 -> 139998005656256
	139998005656256 [label=AccumulateGrad]
	139998005656160 -> 139998005656352
	139998005920448 [label="layer2.2.bn2.bias
 (32)" fillcolor=lightblue]
	139998005920448 -> 139998005656160
	139998005656160 [label=AccumulateGrad]
	139998005656400 -> 139998005656448
	139998005656832 -> 139998005656928
	139998005921568 [label="layer3.0.conv1.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	139998005921568 -> 139998005656832
	139998005656832 [label=AccumulateGrad]
	139998005657216 -> 139998005657120
	139998005921648 [label="layer3.0.bn1.weight
 (64)" fillcolor=lightblue]
	139998005921648 -> 139998005657216
	139998005657216 [label=AccumulateGrad]
	139998005657360 -> 139998005657120
	139998005921728 [label="layer3.0.bn1.bias
 (64)" fillcolor=lightblue]
	139998005921728 -> 139998005657360
	139998005657360 [label=AccumulateGrad]
	139998005657456 -> 139998104765392
	139998005922128 [label="layer3.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139998005922128 -> 139998005657456
	139998005657456 [label=AccumulateGrad]
	140002422440672 -> 139998105010032
	139998005922208 [label="layer3.0.bn2.weight
 (64)" fillcolor=lightblue]
	139998005922208 -> 140002422440672
	140002422440672 [label=AccumulateGrad]
	140002422440480 -> 139998105010032
	139998005922288 [label="layer3.0.bn2.bias
 (64)" fillcolor=lightblue]
	139998005922288 -> 140002422440480
	140002422440480 [label=AccumulateGrad]
	139998105158080 -> 139998105158176
	139998105158080 [label=NativeBatchNormBackward0]
	140002422440576 -> 139998105158080
	140002422440576 [label=ConvolutionBackward0]
	139998005656544 -> 140002422440576
	139998005656640 -> 140002422440576
	139998005920928 [label="layer3.0.downsample.0.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	139998005920928 -> 139998005656640
	139998005656640 [label=AccumulateGrad]
	139998005656736 -> 139998105158080
	139998005921008 [label="layer3.0.downsample.1.weight
 (64)" fillcolor=lightblue]
	139998005921008 -> 139998005656736
	139998005656736 [label=AccumulateGrad]
	139998005657504 -> 139998105158080
	139998005921088 [label="layer3.0.downsample.1.bias
 (64)" fillcolor=lightblue]
	139998005921088 -> 139998005657504
	139998005657504 [label=AccumulateGrad]
	139998105167632 -> 139998006467296
	139998005922768 [label="layer3.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139998005922768 -> 139998105167632
	139998105167632 [label=AccumulateGrad]
	139998006467200 -> 139998005363184
	139998005922688 [label="layer3.1.bn1.weight
 (64)" fillcolor=lightblue]
	139998005922688 -> 139998006467200
	139998006467200 [label=AccumulateGrad]
	139998005363088 -> 139998005363184
	139998005922848 [label="layer3.1.bn1.bias
 (64)" fillcolor=lightblue]
	139998005922848 -> 139998005363088
	139998005363088 [label=AccumulateGrad]
	139998005363328 -> 139998005363136
	139998005923328 [label="layer3.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139998005923328 -> 139998005363328
	139998005363328 [label=AccumulateGrad]
	139998005363664 -> 139998005363760
	139998005923408 [label="layer3.1.bn2.weight
 (64)" fillcolor=lightblue]
	139998005923408 -> 139998005363664
	139998005363664 [label=AccumulateGrad]
	139998005363712 -> 139998005363760
	139998005923488 [label="layer3.1.bn2.bias
 (64)" fillcolor=lightblue]
	139998005923488 -> 139998005363712
	139998005363712 [label=AccumulateGrad]
	139998005363808 -> 139998005363856
	139998005363520 -> 139998005364048
	139998005923888 [label="layer3.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139998005923888 -> 139998005363520
	139998005363520 [label=AccumulateGrad]
	139998005364096 -> 139998005364144
	139998005923968 [label="layer3.2.bn1.weight
 (64)" fillcolor=lightblue]
	139998005923968 -> 139998005364096
	139998005364096 [label=AccumulateGrad]
	139998005363904 -> 139998005364144
	139998005924048 [label="layer3.2.bn1.bias
 (64)" fillcolor=lightblue]
	139998005924048 -> 139998005363904
	139998005363904 [label=AccumulateGrad]
	139998005364336 -> 139998005364480
	139998005924528 [label="layer3.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139998005924528 -> 139998005364336
	139998005364336 [label=AccumulateGrad]
	139998005364240 -> 139998005364720
	139998005924608 [label="layer3.2.bn2.weight
 (64)" fillcolor=lightblue]
	139998005924608 -> 139998005364240
	139998005364240 [label=AccumulateGrad]
	139998005363952 -> 139998005364720
	139998005924688 [label="layer3.2.bn2.bias
 (64)" fillcolor=lightblue]
	139998005924688 -> 139998005363952
	139998005363952 [label=AccumulateGrad]
	139998005364768 -> 139998005364816
	139998005364672 -> 139998005364960
	139998005364672 [label=TBackward0]
	139998117782976 -> 139998005364672
	139998005925088 [label="fc.weight
 (10, 64)" fillcolor=lightblue]
	139998005925088 -> 139998117782976
	139998117782976 [label=AccumulateGrad]
	139998005364960 -> 139998120774368
}
